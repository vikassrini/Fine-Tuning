{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66bc0f06",
   "metadata": {},
   "source": [
    "##  Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c214dba3-9553-4668-8582-b5edb7c13492",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "%pip install transformers datasets peft accelerate torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367aeaca",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df95981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, Trainer, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e991901",
   "metadata": {},
   "source": [
    "## Pre-Process The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "282f340e-f1ba-4933-af49-0642863c01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_373/3878564307.py:83: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='585' max='585' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [585/585 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.309300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=585, training_loss=0.26500008636050754, metrics={'train_runtime': 120.4137, 'train_samples_per_second': 77.632, 'train_steps_per_second': 4.858, 'total_flos': 2442602081968128.0, 'train_loss': 0.26500008636050754, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_data(data):\n",
    "    tokenized_data = []\n",
    "    for item in data:\n",
    "        # Tokenize the question and context together with offset mapping\n",
    "        inputs = tokenizer(\n",
    "            item['question'],\n",
    "            item['context'],\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_offsets_mapping=True,  # This is crucial\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        offset_mapping = inputs.pop('offset_mapping')  # Extract offset mapping\n",
    "        input_ids = inputs['input_ids'].squeeze()  # Remove batch dimension\n",
    "\n",
    "        # Convert character indices to token indices for the answer\n",
    "        start_char = item['answer_start_index']\n",
    "        end_char = item['answer_end_index']\n",
    "\n",
    "        start_token_idx, end_token_idx = None, None\n",
    "\n",
    "        for i, (start, end) in enumerate(offset_mapping.squeeze().tolist()):\n",
    "            if start_char >= start and start_char < end:\n",
    "                start_token_idx = i\n",
    "            if end_char > start and end_char <= end:\n",
    "                end_token_idx = i\n",
    "                break  # Stop once the end position is found\n",
    "\n",
    "        # Ensure valid token indices\n",
    "        if start_token_idx is None or end_token_idx is None:\n",
    "            continue  # Skip this example if indices are not found\n",
    "\n",
    "        tokenized_data.append({\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'token_type_ids': inputs['token_type_ids'].squeeze(),\n",
    "            'start_positions': torch.tensor([start_token_idx]),\n",
    "            'end_positions': torch.tensor([end_token_idx])\n",
    "        })\n",
    "    \n",
    "    return tokenized_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c66f6",
   "metadata": {},
   "source": [
    "## Load The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1238434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Load your dataset\n",
    "data = load_dataset('qa_london_data.json')\n",
    "\n",
    "# Preprocess the data\n",
    "tokenized_datasets = preprocess_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0359a5",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceb106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"no\",  # Disable evaluation\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,  # Only training dataset\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b95765",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49a99e11-a3de-42a7-a245-a352f1e70bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert/tokenizer_config.json',\n",
       " './fine_tuned_bert/special_tokens_map.json',\n",
       " './fine_tuned_bert/vocab.txt',\n",
       " './fine_tuned_bert/added_tokens.json',\n",
       " './fine_tuned_bert/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a2aeca",
   "metadata": {},
   "source": [
    "## Test The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2ebe37-fbc6-4093-bbdb-497b8ac50b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9995142817497253, 'start': 67, 'end': 78, 'answer': 'guided tour'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py:391: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model\n",
    "qa_pipeline = pipeline(\"question-answering\", model=\"./fine_tuned_bert\", tokenizer=\"./fine_tuned_bert\")\n",
    "\n",
    "# Test on a sample question\n",
    "result = qa_pipeline({\n",
    "    \"question\": \"To which category does the Christmas Lights by Night Open-Top Bus Tour belong?\",\n",
    "    \"context\": \"Christmas Lights by Night Open-Top Bus Tour is an activity of type guided tour. It lasts 1.5 hours...\"\n",
    "})\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
